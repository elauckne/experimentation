{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy NLP\n",
    "\n",
    "Notebook for evaluating the functionality of the NLP framework spaCy\n",
    "\n",
    "Comparison with further frameworks:\n",
    "[Comparison of Top 6 Python NLP Libraries](https://medium.com/activewizards-machine-learning-company/comparison-of-top-6-python-nlp-libraries-c4ce160237eb)\n",
    "\n",
    "- Tokenization\n",
    "- Named Entity Recognition\n",
    "- Word Vectors and Similarity\n",
    "- Integration with sklearn\n",
    "\n",
    "Author: Enrico Lauckner ([github.com/elauckne](github.com/elauckne))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure spaCy\n",
    "\n",
    "Load Language Model for German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: de_core_news_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.0.0/de_core_news_sm-2.0.0.tar.gz#egg=de_core_news_sm==2.0.0 in c:\\users\\elauckner\\anaconda3\\lib\\site-packages (2.0.0)\n",
      "\n",
      "    Linking successful\n",
      "    C:\\Users\\elauckner\\Anaconda3\\lib\\site-packages\\de_core_news_sm -->\n",
      "    C:\\Users\\elauckner\\Anaconda3\\lib\\site-packages\\spacy\\data\\de_core_news_sm\n",
      "\n",
      "    You can now load the model via spacy.load('de_core_news_sm')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ihre Berechtigungen reichen nicht aus, um diesen Vorgang auszuführen.\n"
     ]
    }
   ],
   "source": [
    "# Run as administrator in terminal\n",
    "! python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.de.German at 0x20609690a58>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('de_core_news_sm')\n",
    "nlp "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part-of-speech tags and dependencies\n",
    "\n",
    "* Text: The original word text.\n",
    "* Lemma: The base form of the word.\n",
    "* POS: The simple part-of-speech tag.\n",
    "* Tag: The detailed part-of-speech tag.\n",
    "* Dep: Syntactic dependency, i.e. the relation between tokens.\n",
    "* Shape: The word shape – capitalisation, punctuation, digits.\n",
    "* is alpha: Is the token an alpha character?\n",
    "* is stop: Is the token part of a stop list, i.e. the most common words of the language?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Eine große Welle an Berichten schwappte von Freitag an durch alle Medien.'\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eine Eine DET ART nk Xxxx True False\n",
      "große groß ADJ ADJA nk xxxx True True\n",
      "Welle Welle NOUN NN sb Xxxxx True False\n",
      "an an ADP APPR mnr xx True True\n",
      "Berichten Bericht NOUN NN nk Xxxxx True False\n",
      "schwappte schwappen VERB VVFIN ROOT xxxx True False\n",
      "von von ADP APPR mo xxx True True\n",
      "Freitag Freitag NOUN NN nk Xxxxx True False\n",
      "an an ADP APZR ac xx True True\n",
      "durch durch ADP APPR mo xxxx True True\n",
      "alle all DET PIAT nk xxxx True True\n",
      "Medien Medium NOUN NN nk Xxxxx True False\n",
      ". . PUNCT $. punct . False False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "          token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'determiner'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"DET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'noun kernel element'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"nk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ent = 'Emanuel Macron (Präsident von Frankreich) trifft den Geschäftsführer von Microsoft.'\n",
    "doc = nlp(text_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emanuel Macron 0 14 PER\n",
      "Frankreich 30 40 LOC\n",
      "Microsoft 73 82 ORG\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize entities, result is displayed on http://localhost:5000/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Serving on port 5000...\n",
      "    Using the 'ent' visualizer\n",
      "\n",
      "\n",
      "    Shutting down server on port 5000.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "displacy.serve(doc, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vectors and Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geldwäsche Geldwäsche 1.0\n",
      "Geldwäsche Euro 0.5938709\n",
      "Geldwäsche Bareinzahlung 0.49834657\n",
      "Geldwäsche Betrug 0.49147055\n",
      "Euro Geldwäsche 0.5938709\n",
      "Euro Euro 1.0\n",
      "Euro Bareinzahlung 0.7818639\n",
      "Euro Betrug 0.6534954\n",
      "Bareinzahlung Geldwäsche 0.49834657\n",
      "Bareinzahlung Euro 0.7818639\n",
      "Bareinzahlung Bareinzahlung 1.0\n",
      "Bareinzahlung Betrug 0.6406553\n",
      "Betrug Geldwäsche 0.49147055\n",
      "Betrug Euro 0.6534954\n",
      "Betrug Bareinzahlung 0.6406553\n",
      "Betrug Betrug 1.0\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp('Geldwäsche Euro Bareinzahlung Betrug')\n",
    "\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration with sklearn\n",
    "\n",
    "[Tutorial](https://www.analyticsvidhya.com/blog/2017/04/natural-language-processing-made-easy-using-spacy-%E2%80%8Bin-python/)\n",
    "\n",
    "Sentiment Classification Sample Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [('Ich liebe dieses Sandwich.', 'pos'),          \n",
    "         ('das war ein toller ort!', 'pos'),\n",
    "         ('ich mochte die Biere.', 'pos'),\n",
    "         ('das ist meine beste Arbeit.', 'pos'),\n",
    "         (\"was für eine großartige aussicht\", 'pos'),\n",
    "         ('Ich mag dieses Restaurant nicht', 'neg'),\n",
    "         ('Ich hab genug von dem Zeug.', 'neg'),\n",
    "         (\"Ich komm damit nicht klar\", 'neg'),\n",
    "         ('Er ist mein Erzfeind!', 'neg'),          \n",
    "         ('mein chef ist schrecklich.', 'neg')] \n",
    "test =   [('das bier war gut.', 'pos'),     \n",
    "         ('meine arbeit macht mir keinen spaß', 'neg'),\n",
    "         (\"ich fühle mich nicht dandy heute.\", 'neg'),\n",
    "         (\"ich fühle micht nicht gut!\", 'neg'),\n",
    "         ('Gary ist mein bester freund.', 'pos'),\n",
    "         (\"Ich kann nciht glauben, dass ich das tue.\", 'neg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "from spacy.lang.de import German\n",
    "from spacy.lang.de.stop_words import STOP_WORDS as stopwords_ger\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = German()\n",
    "punctuations = string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create spacy tokenizer that parses a sentence and generates tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(sentence):\n",
    "    tokens = parser(sentence)\n",
    "    tokens = [tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_ for tok in tokens]\n",
    "    tokens = [tok for tok in tokens if (tok not in stopwords_ger and tok not in punctuations)]     \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,2))\n",
    "classifier = LinearSVC()\n",
    "\n",
    "pipe = Pipeline([('vectorizer', vectorizer),\n",
    "                 ('classifier', classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('das bier war gut.', 'pos') neg\n",
      "('meine arbeit macht mir keinen spaß', 'neg') pos\n",
      "('ich fühle mich nicht dandy heute.', 'neg') neg\n",
      "('ich fühle micht nicht gut!', 'pos') neg\n",
      "('Gary ist mein bester freund.', 'pos') neg\n",
      "('Ich kann nciht glauben, dass ich das tue.', 'neg') neg\n"
     ]
    }
   ],
   "source": [
    "pipe.fit([x[0] for x in train], [x[1] for x in train]) \n",
    "pred_data = pipe.predict([x[0] for x in test]) \n",
    "for (sample, pred) in zip(test, pred_data):\n",
    "    print(sample, pred )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_score([x[1] for x in test], pred_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
